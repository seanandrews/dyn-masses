{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8/3 Notes\n",
    "\n",
    "Mass and ages are intrinsically imprecise, they have intrinsic uncertanties that are independent from models:\n",
    "* Calculating stellar mass will give you an uncertainty within 20-30% of the value (e.g 1 $M_{\\odot} \\pm 0.2 M_{\\odot}$)\n",
    "* Calculating a stellar age will give you an uncertainty 5-10x (e.g 1 Myr ± 5 Myr)\n",
    "\n",
    "Magnetic fields serve one portion of the uncertainty–serve as better indicator for lower mass stars (1.3 - 1.4 $M_{\\odot}$)\n",
    "\n",
    "This is the threshold for when stars are dominated by radiative processes (as opposed to convection)\n",
    "\n",
    "Incorporate magnetic models into account will sway MIST distributions, and will better correct them\n",
    "\n",
    "discoveries in star formation informs those interested in galactic formation/stellar populations\n",
    "\n",
    "\n",
    "theory that stars in a cluster are scattered around a particular isochrone (different ages): are they really different ages? or is it observational uncertainty? or model uncertainty?\n",
    "\n",
    "magnetic fields arise in lower mass stars because rotating ionized fluid (basic components of star) will cause a magnetic field\n",
    "\n",
    "In planet formation, assume that cluster was born at roughly the same time\n",
    "End goal:\n",
    "\n",
    "In stellar population/demographic studies, uncertain if clusters were actually born at the same time\n",
    "Case study: Upper Scorpius (~1Myr)\n",
    "\n",
    "\n",
    "How to resolve differences between mag model and MIST?\n",
    "Use an independent mass calculation (protoplanetary disks) and figure out how much magnetic field you need to predict the right age\n",
    "\n",
    "Finding out the right age is wildly important, because then you have more time to account for when it comes to planet formation (planetary disks have more time to make a planet, elements have longer time to decay, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8/14 Notes\n",
    "Planetary disk is similar to an MRI scan, except the third dimension is not spatial, it is velocity\n",
    "\n",
    "The combination of channel maps at different velocities shows the whole picture of the emission of the disk in 2D\n",
    "\n",
    "Seeing disk at an angle- if we saw disk from straight on, we would not be able to see it- it would look like constant velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9/3 Notes\n",
    "The TW Hydra data does not have much rotation, it was captured nearly face on— therefore, the two peaks in velocity are closer?\n",
    "\n",
    "* Peaks are dependent on inclination and mass of star\n",
    "    * Farther if inclination is high and mass is high\n",
    "        \n",
    "__Goal:__ Describe spectral line emission by varying parameters\n",
    "Using 'fake data' and feeding it into a model to make inferences on the masses of stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9/10 Notes\n",
    "* On __doppler width__:\n",
    "    * Intrinsically related to the spectrum, little relation to channel map velocities/systemic velocities\n",
    "    * The complex doppler formula is matter of preference (varies within astrophysics community)\n",
    "        * If you want to measure turbulence/its effects, use the more complex one\n",
    "        * Simpler one is more practical, has more clear relationship with radius\n",
    "    \n",
    "* The `bmaj` argument in the channel maps controls the intrinsic spatial imperfections of the instrument\n",
    "    * Factors like atmosphere, % can blur the image\n",
    "    \n",
    "* `r0`, `r_l` is the min/max related to the power law\n",
    "* `r_min` and `r_max` are quasi-archaic, related to variables like `Tbeps`, but generally define the boundaries of the disk in the grid\n",
    "\n",
    "* Polar angle is what does the spinning/rotating\n",
    "    * Define polar angle based on the red-shift of the major axis\n",
    "        * if PA = 0, then major axis will show North to South\n",
    "        * if PA = 90, then major axis will be East to West\n",
    "        \n",
    "* Astrophysics community assumes that disks are azimuthally symmetric (symmetry around polar angle; rotation)\n",
    "    * False\n",
    "        \n",
    "* simple_disk.py model doesn't account for specific transitions within individual atoms\n",
    "    * identify quantum-mechanically (SAHA equation?) what the elements are doing, identify transitions and how many photons are being emitted by each molecule\n",
    "    \n",
    "    \n",
    "* Current side project: developing radio transfer code that accounts for these smaller/more specific parameters in QM\n",
    "    * Preliminary testing of model works well, but it messes up on the doppler linewidths\n",
    "    * Predicts masses well though, mass affects structure of emission\n",
    "    * tricky part is degeneracies:\n",
    "        * mass and inclination are intertwined (linearly-dependent transformations?), they can accomplish the same things and it's hard to tell which one is doing what\n",
    "        \n",
    "        \n",
    "* optical depth - how far you can see into the disk\n",
    "    * changes with position\n",
    "    * Large optical depth = optically thick -- light is readily absorbed (large probing region).\n",
    "        * If gas is __optically thick__, then a photon will interact __more__ with particles before it finally escapes from the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9/14\n",
    "\n",
    "* Dr. Andrews/Teague have been working on statistical inferencing of the spectral line model\n",
    "    * __Statistical inferencing:__ statistical techniques (in this case, Bayesian analysis) that tell us how well the model measures individual parameters ($m_{*}$, $\\rm{T}_b$, $\\Delta v$, etc.)\n",
    "        * Posterior Probability- Bayesian probability of the accuracy of the models given data/outside information\n",
    "            * __Bayes Theorem:__ $$\\Pr ( M \\mid D ) = \\Pr(D \\mid M) * \\Pr(M) \\ / \\Pr(D)$$\n",
    "            * _In words: The probability of the model given the data is equal to the probability of the data given the model (__likelihood__) multiplied by the probability of the data (__prior__) divided by the probability of the data (__Bayesian evidence__)_\n",
    "                * __likelihood:__ ($\\rm{L}$) The probability of the model given the data\n",
    "                    * statistical inferencing is done in natural log because it is assumed that probabilities are gaussian, and the natural log of a gaussian is ~ roughly ~ the area under the curve\n",
    "                    * Found through __chi-squared analysis__\n",
    "                        * $\\chi^2=(D-M)^2 \\ / \\ (\\rm{noise})$\n",
    "                            * roughly, subtract model channel maps from data channel maps\n",
    "                        * $\\ln(\\rm{L}) = \\Sigma_{i=0} = -\\chi^2 \\ / \\ 2$ (__double check__)\n",
    "                            * sum over visibilities (for now, all channel maps for data and model)\n",
    "                * __prior:__ $(\\Pr(M))$ the assumptions we make about the data\n",
    "                    * assumptions like the range of the data (e.g. $0-1000 \\rm{K}$), the gaussian distribution of the data (e.g. $370 \\pm 10 \\ \\rm{K}$)\n",
    "                * __Bayesian evidence:__ $(\\Pr(D))$ hard to measure, but not necessary\n",
    "                    * __MCMC algorithms__ do not need this quantity to work\n",
    "        * In reality, $\\Pr ( M \\mid D ) \\sim \\Pr(D \\mid M) * \\Pr(M) \\ / \\Pr(D)$\n",
    "            * Because $\\Pr(D)$ is hard to measure, we use __Markov Chain Monte Carlo (MCMC)__ to weigh this bayesian probability equation in relation to proportions of the numerator, not exact values\n",
    "            * __Markov Chain Monte Carlo__  uses a __Markov chain__ to sample probability at random points (random walks)\n",
    "                * A __Monte Carlo__ algorithm is a way of randomly sampling a distribution to estimate the distributions of parameters given a set of observations\n",
    "                * A __Markov chain__ looks at the ratio of probabilities between current chain and last chain to determine where to sample probability next\n",
    "                    * More specifically, the next sample is determined by the correlation between the area of the PDF and the ratio\n",
    "                        * Algorithm finds ratios that contribute significantly to the area of the PDF\n",
    "                * A variant of MCMC (that will most likely be the algorithm used on models) is the __affine invariant ensemble sampler__ (`emcee` package)\n",
    "                    * Essentially, multiple Markov chains occuring at once for faster processing\n",
    "                        * Can specify number of chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9/21\n",
    "* Code that provides an output for 'fake data' `analyze_fit_emcee.py`\n",
    "    * Goal: do we recover what we put in? We know the truth for the parameters, but does emcee handle everything correctly?\n",
    "    * Fake data is generated with _no noise_ (no noise meaning that the typical noise from an interferometer is not accounted for (in the form of gaussians), only vague uncertainties\n",
    "    * Code is slow on two accounts:\n",
    "        * Making a cube to store data\n",
    "            * __broadcasting__ arrays: handling arrays of different sizes for computation (NumPy)\n",
    "                * i.e. matrix * scalar number (larger than 1)\n",
    "            * computationally taxing\n",
    "        * Post processing/interpolation for 1e6 different arrays for the likelihood calculation\n",
    "        * Is possible to make it faster using Numba\n",
    "            * Basically takes python code and makes it faster—cluster/machine optimized\n",
    "    \n",
    "* __Next steps:__\n",
    "    * Testing a real dataset\n",
    "        * Advantage: figure out what the errors are, sooner\n",
    "        * Disadvantage: we don't know 'the truth'/what we're looking for, so we are subject to lots of biases\n",
    "    * Technical biases\n",
    "        * Training data to conform to the truth\n",
    "        * make certain calibrations to improve computational speed\n",
    "            * time-averaging: taking larger samples in time (i.e not ever 6 seconds, but every 30 seconds)\n",
    "                * Question: Does it make sense to time-average? Do we still retain the same amount of information? __how to optimize__\n",
    "\n",
    "* __Long term:__\n",
    "    * Is it best to find out ideal conditions to collect interferometer data to probe for disk masses?\n",
    "    * Or, is it best to figure out what a model of dynamical masses actually looks like and the errors associated with it?\n",
    "    * Exploring both simultaneously (Teague, Andrews, and myself)\n",
    "\n",
    "* __Miscellaneous:__\n",
    "    * Spectral resolution: frequency/velocity at which you take measurements of the sky with interferometry\n",
    "        * One technique to increase resolution: VLBI (Very-Long Baseline Interferometry)\n",
    "    * Optimum data collection: higher spectral resolution\n",
    "        * Caveats:\n",
    "            * Telescope time is competitive\n",
    "                * Observations fall into two camps:\n",
    "                    * Snapshot (1 sec-1 min of sky)\n",
    "                    * Medium length (20-30 minutes)\n",
    "                    * Long (1-5 hrs)\n",
    "            * Higher spectral resolution = higher noise (similar to fourier transform analysis)\n",
    "    * Spectral continuum = 1 channel map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9/24 Notes\n",
    "__Takeaways from Interferometry Primer:__\n",
    "* Fourier transform of point source will be constant\n",
    "* Fourier transform of an elliptical gaussian distribution will be a rotated elliptical gaussian distribution\n",
    "* Fourier transforms of sharp edges (uniform disk) are bessel functions\n",
    "\n",
    "__(narrow features turn into wide features)__ = things captured at smaller baselines represent large-scale structure and vice versa\n",
    "* more angular resolution at larger baselines, but usually need many baselines of varying sizes to fully reconstruct the angular distribution of an object\n",
    "\n",
    "__Resolution of interferometer:__ $\\theta_b \\sim \\lambda/d$ where d is either telescope diameter, or baseline b/w antennas\n",
    "* For small scale data (high baseline), the telescopes are separated by ~16 km, gets you a resolution of ~ 0.03 arcsec\n",
    "* For large structure data (low baseline), telescopes are separated by 0.5-2 km\n",
    "\n",
    "\n",
    "* On a plot of visibility vs. baseline, there is less visibility as you go further in baseline b/c there is less emission at a finer resolution\n",
    "\n",
    "* However, visibility data is limited, you can't measure all (u, v) values\n",
    "\n",
    "* Baselines within interferometry presents limitations on how large you can go\n",
    "    * only really a problem for galaxy/stellar research, they combat this by taking interferometer data (multiple telescopes) and add it to the source from one telescope (to get extended view)\n",
    "    \n",
    "Likelihood calculation is possible, the post processing is the difficult part\n",
    "\n",
    "__Problems to overcome in post processing of likelihood calculation:__\n",
    "1. Because there are window functions and spectral response functions embedded into data, each channel map is not independent\n",
    "    * channel map i and i+1 are covariant with one another, and the fake data needs to also conform to this using some kind of covariance matrix\n",
    "2. Alma does not record data in an independent fixed reference frame\n",
    "    * Uses the __LSRK (Kinematic Local Standard of Rest)__\n",
    "        * Average motion of material in the Milky Way in the neighborhood of the Sun (stars in radius 100 pc from the Sun) [wiki](https://en.wikipedia.org/wiki/Local_standard_of_rest#:~:text=In%20astronomy%2C%20the%20local%20standard,material%20is%20not%20precisely%20circular.)\n",
    "            * In the range of 202–241 km/s\n",
    "    * Easy to fix in theory, apply transformation to data\n",
    "        * __Except__ applying transformations to hundreds of data cubes takes an enormously long time\n",
    "        * Because of the convolution and interpolation of the transformation, it biases parameters\n",
    "            * not dynamical mass\n",
    "    * One way to fix:\n",
    "        * Bin the observations to one channel map (fail)\n",
    "            * Only transform the halfway point of the observation dataset to standard coordinates\n",
    "                * covariance matrix will have no covariance, because just single channel map\n",
    "                * But applying window function makes the matrix invertible (poorly conditioned matrix)\n",
    "        * Bin the observations to two channel maps (successful)\n",
    "            * Creates a tri-diagonal covariance matrix, which is invertible\n",
    "        * __Consequence__: you need 2x better resolution to do this approach\n",
    "            * This makes posterior widths narrower than they should be, creating precision that you don't have\n",
    "\n",
    "__Processes that take the longest:__\n",
    "1. Generating cubes\n",
    "    * can potentially speed this up\n",
    "2. FT of cubes\n",
    "3. Interpolation of spectra\n",
    "    * Can speed this up using Numba\n",
    "        * instead of broadcasting arrays, turn them into loops\n",
    "\n",
    "\n",
    "### Resources:\n",
    "[Geometry of Interferometry](https://drive.google.com/file/d/1snWR17-5llqqoY5BHPW4I9iFJLAdYCK2/view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10/1 Notes\n",
    "\n",
    "## Process\n",
    "UVfits file is fake data with frequencies\n",
    "1. process it so that it reads in terms of velocities, reads in correct coordinates, and identifies the min-max boundaries for fake data\n",
    "2. calculate covariance matrix for chi-squared analysis later\n",
    "3. load template model frequencies\n",
    "\n",
    "4. __likelihood calculation__\n",
    "    * generate model cube visibilities according to inputted parameters/default values from passer\n",
    "    * sample fourier transforms of visibilities onto u, v points\n",
    "    * convolute FT of visibilities? (frequency-domain) with FT of autocorrelation of the Hann window function (also called __spectral response function (SRF)__, frequency-domain)\n",
    "        * SRF makes things blurry, because the FT of the window function can only account for so much data\n",
    "    * interpolation of channel maps\n",
    "        * figure out LSRK frequencies that are relevant, and interpolate frequencies from data to frequencies from model \n",
    "    * in current version, all of this is done for only the middle channel map of the observation (~450 seconds)\n",
    "\n",
    "* **but repeat #4 for every timestamp (for 900 second observation, averaged every 30 seconds, so 30x)**\n",
    "\n",
    "## Misc\n",
    "* UVfits files have a 4D array\n",
    "    * for each separate u & v coordinate\n",
    "    * first two indices [0, 1] are real and imaginary visibilities\n",
    "    * the second index is the weights (1/variance of each file)\n",
    "    * third index is polarization\n",
    "        * left/right or x-y polarizations?\n",
    "        * treat these as independent measurements bc they're usually related to measuring device (interferometer)\n",
    "\n",
    "* where to get constant values from (e.g. 12CO J=2-1 transition)\n",
    "    * NASA JPL has a database called ['LAMBDA'](https://lambda.gsfc.nasa.gov/product/planck/curr/planck_prod_esa.cfm)\n",
    "* topocentric- Earth reference frame\n",
    "* the data/model/covariance matrix must be binned by a factor of $\\geq$ 2\n",
    "    * covariance matrix is non-invertible if it is not\n",
    "        * has a high condition number, which dictates how invertible it is\n",
    "        * if there is no convolution, then there is a high amount of noise that is introduced into log likelihood\n",
    "    * __what this means:__ things must be binned because channels are intrinsically blurred by SRF\n",
    "        * spectra from channel maps are not independent, they are dependent on their i+1 counterparts\n",
    "    * if there are 100 frequencies, then binning by 2 would result in 50 frequencies\n",
    "    * binning is a maximization problem, you want to bin as little as possible, but the more you bin, the more independent the channels become\n",
    "        * more bins, less covariance/more independence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10/5 Notes\n",
    "\n",
    "* __On LSRK__\n",
    "    * LSRK frame is supposed to represent fixed reference frame wrt Earth and disk\n",
    "        * only thing to account for is projected velocities?\n",
    "    * We have more information for LSRK velocities because of overlapping frequencies\n",
    "        * ALMA records data into fixed topocentric channels, but LSRK frequencies change from channel to channel\n",
    "            * Each channel has ~30 LSRK velocities, and the LSRK velocities change with time\n",
    "            * Converting from TOPO to LSRK requires regridding:\n",
    "                * Actual transformation (geometric equation)\n",
    "                * Taking 100 channels and 30 integrations (~3,000 LSRK velocities) and interpolating them with the frequencies that we want (TOPO converted LSRK)\n",
    "                * Finding desired TOPO frequencies and finding interpolated LSRK velocities for each channel\n",
    "* __On Data Treatment__\n",
    "    * model that generates cube is oversampled because of the convolution\n",
    "        * supposed to mimic 'infinite' signal stream that is being fed into correlator\n",
    "        * theory is that it leads to a more realistic/accurate convolution product\n",
    "    * SRF function (Hann vs. simplified)\n",
    "        * SRF function is what causes covariance from channel to channel\n",
    "            * Visibilities are not something that can be easily broken down/taken out/be independent (only in theory, but not in practice)\n",
    "                * Correlator automatically correlates visibilities with SRF\n",
    "        * visibilities don't have gradients from channel to channel?, which is why we can use simplified SRF\n",
    "            * simplified SRF is essentially delta functions at 0.25, 0.5, and 0.75\n",
    "                * the FWHM of the Hann SRF function is 2 channels (spectral resolution)\n",
    "        \n",
    "    * Log-likelihood for known quantities should render a $\\chi^2$ value of 0\n",
    "        * except there's a fraction of 0.71 that might be due to convolution steps/oversampling\n",
    "        * but this is really insignificant, because it's 0.71 of the data out of 1 million\n",
    "* On molecular compounds\n",
    "    * $^{12}\\rm{CO}$ is normal carbon monoxide\n",
    "    * $\\rm{H}^2$ is hydrogen gas (most abundant)\n",
    "    * We don't use $\\rm{H}^2$ for multiple reasons:\n",
    "        * it is pure, does not have rotational transition states due to how symmetric of a molecule it is\n",
    "        * no magnetic moment, has quadropole terms in its moments that are far weaker than monopole/dipole\n",
    "    * Use $^{12}\\rm{CO}$ for multiple reasons:\n",
    "        * Second most abundant element in disks\n",
    "        * Is optically thick\n",
    "            * Has lots of emission from only a little bit of gas\n",
    "        * Bright even at small radii\n",
    "            * easier to reconstruct for velocity measurements/dynamical masses\n",
    "        * Specifically J=2-1 is useful because Earth's atmosphere is most transparent with 2-1\n",
    "            * 1-0 is too hard to observe, is too close to oxygen spectar that is present in Earth's atmosphere\n",
    "            * 3-2 is also used, but (i forget why not)\n",
    "    * We say $^{12}\\rm{CO}$ becuase it is an isotopologue of carbon\n",
    "* On larger scale project:\n",
    "    * We're hoping that post-processing of log-likelihood has a better result than typical practices that don't do this\n",
    "    * Need to stress test model for different scenarios in which to use the model/\n",
    "    * Reduction of data by time-averaging\n",
    "        * help advocate for this way of thinking (exploration), and also helps people make better inferences (inferencing)\n",
    "    * `dynesty` might be better than `emcee` for calculating posteriors (in a more mathematical way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10/8 Notes\n",
    "\n",
    "* Next steps: run many different inferences on model, see how outside inferences impact the fit of the model\n",
    "    * But we also want to know what the intrinsic biases of the model are, so that people can make good inferences\n",
    "    * By running no noise into the model, we should be getting back the input parameters that we put in\n",
    "    * By running noise into the model, we find out what the biases of the model are\n",
    "        * Noise is likely to bias the widths (errors) of the parameters, but can also bias the peaks (mean value for distribution)\n",
    "        * We are unsure what the noise should look like (noise is approximated to be gaussian, a la central limit theorem)\n",
    "            * Just a gaussian dist. applied to the real and imaginary visibilities\n",
    "            * In theory, we could run tests on the noise to see how the noise is shaped, but takes too long?\n",
    "\n",
    "\n",
    "* My next steps:\n",
    "    * Modeling corrupted data (via a sampling method, either `emcee` or `dynesty`)\n",
    "        * need to see which one to use first before actually modeling\n",
    "    * Seeing how `dynesty` works for our data\n",
    "        * MCMC (`emcee`) has trouble working with many dimensions (parameters)\n",
    "            * Can sometimes get stuck in parameter space/all parameters are largely independent from one another, and `emcee` usually works better with more correlated parameters?\n",
    "                * need a way to separate posterior sampling\n",
    "            * `dynesty` uses __nested sampling__ to calculate the convergence of posterior probabilities in parameter space\n",
    "                * does not use random walks, it randomly chooses parameters\n",
    "                    * creates a nest around probable regions, and rejects values outside that 'nest'\n",
    "                * also can calculate priors (evidence) unlike `emcee`\n",
    "            * coding-wise, the documentation/programming is more verbose than `emcee`\n",
    "        * eventually will run this with fake noiseless visibilities to see how it compares to `emcee`\n",
    "            * need to figure out how many points to sample from, if burn-in is required, etc.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline\n",
    "---\n",
    "* **Week 1:** Learning about HR Diagram\n",
    "   * Goal: familiarize with astropy/mass tracks/isochrones\n",
    "   * _Frustration with not knowing how to code correctly_\n",
    "* **Week 2:** Addendum of HR Diagram about interpolation, double-tracking problem  \n",
    "   * Goal: figure out how to segment the interpolation of luminosity and temperature to get mass and age  \n",
    "* **Week 3:** Playing around with griddata and arrays\n",
    "   * Goal: use `scipy.interpolate.griddata` to make a gridded HR diagram/image plot based on teff, lum, and mass/age\n",
    "* **Week 4:** Playing around with imshow and plots, new F16 model\n",
    "   * Goal: Make a HR diagram image plot, colormap and normalize it, and align the mass tracks and isochrones to it.\n",
    "       * Completed: (7/28) MIST colormaps are verified by mass tracks and isochrones  \n",
    "   * Goal: Do the same grid/plotting with new F16 model\n",
    "       * Completed: (7/30) F16 colormaps are verified only by mass tracks (working on isochrones)\n",
    "* **Week 5:** Probability Distribution Functions and SPOTS\n",
    "   * Goal: Make code that computes PDFs for MIST\n",
    "   * Goal: Make code that computes PDFs for SPOTS\n",
    "   * Goal: Compare PDFs of MIST and SPOTS\n",
    "       * Completed: (8/4) Made SPOTS Model mass/age plots, and rendered PDF for SPOTS\n",
    "* **Week 6:** Learning about cube data for protoplanetary disks\n",
    "    * Goal: Write code that converts surface brightness to brightness temperature\n",
    "        * Completed: (8/14) Wrote brighttemp.py, converted plot from surface brightness to brightness temperature\n",
    "        * _Frustration with converting janskys to cms units_\n",
    "    * Goal: Make 10x10 channel maps for different velocities of HD163296 disk\n",
    "        * Completed: (8/11) Channel maps\n",
    "* **Week 7:** Learning about different metrics for cube data\n",
    "    * Goal: Learn how to manipulate data for different types of plots:\n",
    "        * Completed: (8/17) Spectrum, Moment-0 map\n",
    "        * Completed: (8/18) Moment-1 map, got bettermoments working (checked moment-0 and moment-1)\n",
    "            * Note: Your moment-1 map differs by a factor of $1/dv$ compared to bettermoments\n",
    "        * _Frustration with moment maps_\n",
    "* **Week 8:** Fixing up moment maps and playing with Gofish\n",
    "    * Goal: Finish up last week moment maps\n",
    "        * Completed: (8/25) Checked moment-2 and moment-8 maps with bettermoments, got colormaps working correctly\n",
    "    * Goal: Replicate basics of gofish notebook for HD163296 disk\n",
    "* **Week 9:**\n",
    "    * Goal: Finish basics of gofish notebook for HD163296 disk\n",
    "        * Completed: (9/2) Gofish analysis on HD163296\n",
    "* **Week 10:**\n",
    "    * Goal: Learning `simple_disk.py` and playing with model\n",
    "* __Week 11:__ Fundamentals of Statistical Inferencing\n",
    "    * Goal: learn how to use `emcee` and more about MCMC\n",
    "    * _Frustrated about how MCMC works/implementing Metropolis-Hastings_\n",
    "* __Week 12:__ Fake data run through `emcee` and interferometry primer\n",
    "    * Goal: Learn more about interferometry\n",
    "    * Clean up analysis script/work on visualizations\n",
    "    * _Frustrated on how difficult interferometry is_\n",
    "* __Week 13:__ Understand `logL_test.py` and its workings\n",
    "    * Goal: Also learn how to use Numba\n",
    "    * _Frustrated on not knowing what your role in the project is_\n",
    "* __Week 14:__ Numba speed-up\n",
    "    * Goal: Learn how to use Numba\n",
    "    * Goal: Experiment with how we can speed up interpolation\n",
    "    * Completed (10/8): Basic 1D linear interpolator (with simple data set)\n",
    "    * Completed (10/9): Got Numba to run 4x faster than SciPy interpolator (but not too accurate yet)\n",
    "    Feel like I know how to use Numba now!\n",
    "* __Week 15:__ `dynesty` learning\n",
    "    * Goal: Learn how to use `dynesty`\n",
    "    * Goal: Finish interpolator and make it accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
