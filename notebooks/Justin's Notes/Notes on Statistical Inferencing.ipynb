{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Statistical Inferencing\n",
    "\n",
    "### Probability\n",
    "* __Multivariate Gaussian density__ (this is similar to the exponential of a single variable gaussian PDF)\n",
    "    <blockquote>$$\n",
    "    p(\\vec{x}) \\propto \\exp \\left [ - \\frac{1}{2} (\\vec{x} -\n",
    "        \\vec{\\mu})^\\mathrm{T} \\, \\Sigma ^{-1} \\, (\\vec{x} - \\vec{\\mu})\n",
    "        \\right ]$$\n",
    "\n",
    "    where $\\vec{\\mu}$ is an $N$-dimensional vector position of the mean of the density and $\\Sigma$ is the square N-by-N covariance matrix.\n",
    "    </blockquote>\n",
    "* __covariance:__ measure of the relationship between two random variables\n",
    "    * $\\sigma(x,y)= \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})$\n",
    "* __variance:__ measure of the spread of a sample/population\n",
    "    * $S^2 = \\sum(x_i - \\bar{x}) \\ / \\ n-1$ (for sample)\n",
    "    * Also, the variance can be described as the covariance of a single variable $\\sigma(x, x)$\n",
    "\n",
    "\n",
    "### Ways to Interpolate data\n",
    "* Most basic MCMC: the __Metropolis-Hastings__ algorithm\n",
    "    * One chain, one walker\n",
    "    * Likelihood function, priors, and proposal distribution (walker)\n",
    "        * Data set has certain mean, variance, standard deviation\n",
    "        * The walker finds the values in parameter space to probe (randomly)\n",
    "        * Feeds the points to likelihood function (usually gaussian) and priors\n",
    "            * Distribution of a likelihood function and priors\n",
    "            * These are multiplied together\n",
    "    * [More on Metropolis-Hastings](https://github.com/Joseph94m/MCMC/blob/master/MCMC.ipynb)\n",
    "* __Linear Least squares:__ finding a line that best fits several linear equations\n",
    "    * $A\\vec{x}=b$ has least squares solutions $A^{T}A\\vec{x}=A^Tb$ (normal equation)\n",
    "    * The least squares solution to $A\\vec{x}=b$ is $x^{*}=(A^TA)^{-1}A^Tb$\n",
    "    * The function takes the form: $f_{\\rm{lsfit}}(s) = x_1f_1(s) + ... + x_nf_n(s)$\n",
    "    * __Linear Least Squares Polynomial Fitting:__ a way of finding a polynomial that best fits several coordinates\n",
    "        * Requires __vandermonde matrix__ \n",
    "            * Matrix takes the form: $$\\begin{equation*}\n",
    "    A = \\begin{pmatrix}\n",
    "      1 & x_0 & x_0^2 & \\cdots & x_0^{n-1} &\n",
    "      x_0^{n}\\\\\n",
    "      1 & x_1 & x_1^2 & \\cdots & x_1^{n-1} &\n",
    "      x_1^{n}\\\\\n",
    "      \\vdots & \\vdots  & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
    "      1 & x_n & x_n^2 & \\cdots & x_n^{n-1} &\n",
    "      x_n^{n}\n",
    "    \\end{pmatrix},\n",
    "    \\end{equation*}$$\n",
    "            * Function takes the form:\n",
    "            $$(a_{1}x^{n-1}+a_{2}x^{n-2}... + a_{n-1}x+a_{n}) = y$$\n",
    "            * Input: x, y points\n",
    "            * Output: fitted polynomial\n",
    "            * __How to (basic)__:\n",
    "                * Points: (0, 1), (1, 0), (2/3, 1/2) ($n$ pairs of coordinates)\n",
    "                * Each row of the matrix takes the form: $(a_{1}x^{n-1}+a_{2}x^{n-2}... + a_{n-1}x+a_{n}) = y$\n",
    "                    * EX: First pair of coordinates would be: $(a_{1}0^{2}+a_{2}0^{1}+a_{3}0^0) =1$\n",
    "                    * Forms augmented matrix:\n",
    "                        * $\\begin{pmatrix} 1&0&0&1\\\\ 1&1&1&0\\\\ 1&\\frac{2}{3}&\\frac{4}{9}&\\frac{1}{2}\\end{pmatrix} \\ $ or, more familiarly,  $\\ \\begin{pmatrix}1&0&0\\\\ 1&1&1\\\\ 1&\\frac{2}{3}&\\frac{4}{9}\\end{pmatrix}\\begin{pmatrix}a_0\\\\ a_1\\\\ a_2\\end{pmatrix}=\\begin{pmatrix}1\\\\ 0\\\\ \\frac{1}{2}\\end{pmatrix}$\n",
    "                    * Solution: $= -\\frac{3}{4}x^2-\\frac{1}{4}x+1$\n",
    "    * Code (from [`emcee` notebook](https://emcee.readthedocs.io/en/stable/tutorials/line/)):\n",
    "```python\n",
    "A = np.vander(x, 2) # make vandermonde matrices (shape 50, 2) out of x values\n",
    "C = np.diag(yerr * yerr) # make diagonal matrix out of square of y-errors\n",
    "ATA = np.dot(A.T, A / (yerr ** 2)[:, None]) #finds A^T•A and divides by error squared (C) shape(2,2)\n",
    "cov = np.linalg.inv(ATA) #inverse\n",
    "w = np.linalg.solve(ATA, np.dot(A.T, y / yerr ** 2)) #this is least-squares solution\n",
    "print(\"Least-squares estimates:\")\n",
    "print(\"m = {0:.3f} ± {1:.3f}\".format(w[0], np.sqrt(cov[0, 0]))) #error is sqrt of diagonal of covariance matrix\n",
    "print(\"b = {0:.3f} ± {1:.3f}\".format(w[1], np.sqrt(cov[1, 1])))\n",
    "```\n",
    "    * In math notation:\n",
    "        * $AX=Y$ with solution $w=x^*=[A^T C^{-1} A]^{-1}[A^TC^{-1}Y]$\n",
    "            * $A$ = vandermonde matrix\n",
    "            * $C^{-1}$ = divided by __covariance matrix__: $C(m, b) = \\begin{pmatrix}\\sigma_{b}^2&\\sigma_{mb}^2\\\\ \\sigma_{mb}^2&\\sigma_{m}^2\\end{pmatrix}$\n",
    "                * in the code, $C$ is determined by the diagonal of the error in y\n",
    "            * $Y$ = distribution of y values (with error and added fractional uncertainty)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/14 Meeting Notes\n",
    "\n",
    "* Dr. Andrews/Teague have been working on statistical inferencing of the spectral line model\n",
    "    * __Statistical inferencing:__ statistical techniques (in this case, Bayesian analysis) that tell us how well the model measures individual parameters ($m_{*}$, $\\rm{T}_b$, $\\Delta v$, etc.)\n",
    "        * Posterior Probability- Bayesian probability of the accuracy of the models given data/outside information\n",
    "            * __Bayes Theorem:__ $$\\Pr ( M \\mid D ) = \\Pr(D \\mid M) * \\Pr(M) \\ / \\Pr(D)$$\n",
    "            * _In words: The probability of the model given the data is equal to the probability of the data given the model (__likelihood__) multiplied by the probability of the data (__prior__) divided by the probability of the data (__Bayesian evidence__)_\n",
    "                * __likelihood:__ ($\\rm{L}$) The probability of the model given the data\n",
    "                    * statistical inferencing is done in natural log because it is assumed that probabilities are gaussian, and the natural log of a gaussian is ~ roughly ~ the area under the curve\n",
    "                    * Found through __chi-squared analysis__\n",
    "                        * $\\chi^2=(D-M)^2 \\ / \\ (\\rm{noise})$\n",
    "                            * roughly, subtract model channel maps from data channel maps\n",
    "                        * $\\ln(\\rm{L}) = \\Sigma_{i=0} = -\\chi^2 \\ / \\ 2$ (__double check__)\n",
    "                            * sum over visibilities (for now, all channel maps for data and model)\n",
    "                * __prior:__ $(\\Pr(M))$ the assumptions we make about the data\n",
    "                    * assumptions like the range of the data (e.g. $0-1000 \\rm{K}$), the gaussian distribution of the data (e.g. $370 \\pm 10 \\ \\rm{K}$)\n",
    "                * __Bayesian evidence:__ $(\\Pr(D))$ hard to measure, but not necessary\n",
    "                    * __MCMC algorithms__ do not need this quantity to work\n",
    "        * In reality, $\\Pr ( M \\mid D ) \\sim \\Pr(D \\mid M) * \\Pr(M) \\ / \\Pr(D)$\n",
    "            * Because $\\Pr(D)$ is hard to measure, we use __Markov Chain Monte Carlo (MCMC)__ to weigh this bayesian probability equation in relation to proportions of the numerator, not exact values\n",
    "            * __Markov Chain Monte Carlo__  uses a __Markov chain__ to sample probability at random points (random walks)\n",
    "                * A __Monte Carlo__ algorithm is a way of randomly sampling a distribution to estimate the distributions of parameters given a set of observations\n",
    "                * A __Markov chain__ looks at the ratio of probabilities between current chain and last chain to determine where to sample probability next\n",
    "                    * More specifically, the next sample is determined by the correlation between the area of the PDF and the ratio\n",
    "                        * Algorithm finds ratios that contribute significantly to the area of the PDF\n",
    "                * A variant of MCMC (that will most likely be the algorithm used on models) is the __affine invariant ensemble sampler__ (`emcee` package)\n",
    "                    * Essentially, multiple Markov chains occuring at once for faster processing\n",
    "                        * Can specify number of chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/21 Notes\n",
    "* Code that provides a MCMC output for 'fake data' `analyze_fit_emcee.py`\n",
    "    * Goal: do we recover what we put in? We know the truth for the parameters, but does emcee handle everything correctly?\n",
    "    * Fake data is generated with _no noise_ (no noise meaning that the typical noise from an interferometer is not accounted for (in the form of gaussians), only vague uncertainties\n",
    "    * Code is slow on two accounts:\n",
    "        * Making a cube to store data\n",
    "            * __broadcasting__ arrays: handling arrays of different sizes for computation (NumPy)\n",
    "                * i.e. matrix * scalar number (larger than 1)\n",
    "            * computationally taxing\n",
    "        * Post processing/interpolation for 1e6 different arrays for the likelihood calculation\n",
    "        * Is possible to make it faster using Numba\n",
    "            * Basically takes python code and makes it faster—cluster/machine optimized\n",
    "    \n",
    "* __Next steps:__\n",
    "    * Testing a real dataset\n",
    "        * Advantage: figure out what the errors are, sooner\n",
    "        * Disadvantage: we don't know 'the truth'/what we're looking for, so we are subject to lots of biases\n",
    "    * Technical biases\n",
    "        * Training data to conform to the truth\n",
    "        * make certain calibrations to improve computational speed\n",
    "            * time-averaging: taking larger samples in time (i.e not ever 6 seconds, but every 30 seconds)\n",
    "                * Question: Does it make sense to time-average? Do we still retain the same amount of information? __how to optimize__\n",
    "\n",
    "* __Long term:__\n",
    "    * Is it best to find out ideal conditions to collect interferometer data to probe for disk masses?\n",
    "    * Or, is it best to figure out what a model of dynamical masses actually looks like and the errors associated with it?\n",
    "    * Exploring both simultaneously (Teague, Andrews, and myself)\n",
    "\n",
    "* __Miscellaneous:__\n",
    "    * Spectral resolution: frequency/velocity at which you take measurements of the sky with interferometry\n",
    "        * One technique to increase resolution: VLBI (Very-Long Baseline Interferometry)\n",
    "    * Optimum data collection: higher spectral resolution\n",
    "        * Caveats:\n",
    "            * Telescope time is competitive\n",
    "                * Observations fall into two camps:\n",
    "                    * Snapshot (1 sec-1 min of sky)\n",
    "                    * Medium length (20-30 minutes)\n",
    "                    * Long (1-5 hrs)\n",
    "            * Higher spectral resolution = higher noise (similar to fourier transform analysis)\n",
    "    * Spectral continuum = 1 channel map\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on `logL_test.py` (likelihood calculation)\n",
    "\n",
    "   <blockquote>\n",
    "vis_sample allows you to sample visibilities from a user-supplied sky-brightness image.\n",
    "    \n",
    "   * (u,v) grid points can either be supplied by the user, or can be retrieved from a template uvfits file / measurement set.\n",
    "        * The results can be output either to a uvfits file or returned back to the user (for scripting)\n",
    "   </blockquote>\n",
    "\n",
    "* uvfits file has channel frequencies (what units?)\n",
    "\n",
    "* converts to LSRK velocities in m/s by using $v = c \\ (1-f_{\\rm{native}} \\ / \\ f_{\\rm{rest}})$ (rest frequency of 12CO J=2-1 in [Hz])\n",
    "\n",
    "* find channels that correspond to ±500 m/s\n",
    "\n",
    "Code makes a **deep copy** of the data from `import_data_uvfits` method\n",
    "* shallow copies still refer to original object (more dependent)\n",
    "\t* if you make changes to the original object, the shallow copy will also have changes\n",
    "\n",
    "* deep copies are recursive, and link to themselves (more independent)\n",
    "\n",
    "\n",
    "__On Convolution and Windows:__\n",
    "* Convolution is $(f \\star g)(t) = \\int_{-\\infty}^{\\infty}f(\\tau)*g(t-\\tau)d\\tau$\n",
    "\t* describes how the shape of one function is modified by another\n",
    "\t* basically multiplication and then integration\n",
    "\n",
    "* Window functions are a kind of tapering mechanism (non-constant scaling by a given tapering function, higher order than a scale)\n",
    "\t* mathematical function that is symmetric around middle of the interval (bell shaped), and zero-valued outside of some chosen interval \n",
    "\t* allows better detection of transient events/time-averaging of frequency spectra\n",
    "\n",
    "* signals from interferometer are convoluted with window functions to _aa_ time-averaging of frequency spectra when taking the Fourier Transform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Questions:**\n",
    "* assigning object names to variables? (dvis_native.VV seems like it's self referential)\n",
    "* what is VV?\n",
    "\t* has shape (123, 27090)\n",
    "\t* visibilities per channel (123 channels) with v?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10/1 Notes\n",
    "\n",
    "## Process\n",
    "UVfits file is fake data with frequencies\n",
    "1. process it so that it reads in terms of velocities, reads in correct coordinates, and identifies the min-max boundaries for fake data\n",
    "2. calculate covariance matrix for chi-squared analysis later\n",
    "3. load template model frequencies\n",
    "\n",
    "4. __likelihood calculation__\n",
    "    * generate model cube visibilities according to inputted parameters/default values from passer\n",
    "    * sample fourier transforms of visibilities onto u, v points\n",
    "    * convolute FT of visibilities? (frequency-domain) with FT of autocorrelation of the Hann window function (also called __spectral response function (SRF)__, frequency-domain)\n",
    "        * SRF makes things blurry, because the FT of the window function needs to be sampled for short time?\n",
    "    * interpolation of channel maps\n",
    "        * figure out LSRK frequencies that are relevant, and interpolate frequencies from data to frequencies from model \n",
    "    * in current version, all of this is done for only the middle channel map of the observation (~450 seconds)\n",
    "\n",
    "* **but repeat #4 for every timestamp (for 900 second observation, averaged every 30 seconds, so 30x)**\n",
    "\n",
    "## Misc\n",
    "* UVfits files have a 4D array\n",
    "    * for each separate u & v coordinate\n",
    "    * first two indices [0, 1] are real and imaginary visibilities\n",
    "    * the second index is the weights (1/variance of each file)\n",
    "    * third index is polarization\n",
    "        * left/right or x-y polarizations?\n",
    "        * treat these as independent measurements bc they're usually related to measuring device (interferometer)\n",
    "\n",
    "* where to get constant values from (e.g. 12CO J=2-1 transition)\n",
    "    * NASA JPL has a database called ['LAMBDA'](https://lambda.gsfc.nasa.gov/product/planck/curr/planck_prod_esa.cfm)\n",
    "* topocentric- Earth reference frame\n",
    "* the data/model/covariance matrix must be binned by a factor of $\\geq$ 2\n",
    "    * covariance matrix is non-invertible if it is not\n",
    "        * has a high condition number, which dictates how invertible it is\n",
    "        * if there is no convolution, then there is a high amount of noise that is introduced into log likelihood\n",
    "    * __what this means:__ things must be binned because channels are intrinsically blurred by SRF\n",
    "        * spectra from channel maps are not independent, they are dependent on their i+1 counterparts\n",
    "    * if there are 100 frequencies, then binning by 2 would result in 50 frequencies\n",
    "    * binning is a maximization problem, you want to bin as little as possible, but the more you bin, the more independent the channels become\n",
    "        * more bins, less covariance/more independence\n",
    "\n",
    "## Further Questions\n",
    "* We have more spectral information in LSRK frame? Why is that?\n",
    "* channels become blurred by SRF, but are they independent otherwise?\n",
    "\n",
    "* dvis_native.freqs = frequency for each channel (123 values), in terms of rest frequency frame\n",
    "* vlsrk_native = velocity for each channel, converted to LSRK velocities [m/s] (123 values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10/5 Notes\n",
    "\n",
    "* __On LSRK__\n",
    "    * LSRK frame is supposed to represent fixed reference frame wrt Earth and disk\n",
    "        * only thing to account for is projected velocities?\n",
    "    * We have more information for LSRK velocities because of overlapping frequencies\n",
    "        * ALMA records data into fixed topocentric channels, but LSRK frequencies change from channel to channel\n",
    "            * Each channel has ~30 LSRK velocities, and the LSRK velocities change with time\n",
    "            * Converting from TOPO to LSRK requires regridding:\n",
    "                * Actual transformation (geometric equation)\n",
    "                * Taking 100 channels and 30 integrations (~3,000 LSRK velocities) and interpolating them with the frequencies that we want (TOPO converted LSRK)\n",
    "                * Finding desired TOPO frequencies and finding interpolated LSRK velocities for each channel\n",
    "* __On Data Treatment__\n",
    "    * model that generates cube is oversampled because of the convolution\n",
    "        * supposed to mimic 'infinite' signal stream that is being fed into correlator\n",
    "        * theory is that it leads to a more realistic/accurate convolution product\n",
    "    * SRF function (Hann vs. simplified)\n",
    "        * SRF function is what causes covariance from channel to channel\n",
    "            * Visibilities are not something that can be easily broken down/taken out/be independent (only in theory, but not in practice)\n",
    "                * Correlator automatically correlates visibilities with SRF\n",
    "        * visibilities don't have gradients from channel to channel?, which is why we can use simplified SRF\n",
    "            * simplified SRF is essentially delta functions at 0.25, 0.5, and 0.75\n",
    "                * the FWHM of the Hann SRF function is 2 channels (spectral resolution)\n",
    "        \n",
    "    * Log-likelihood for known quantities should render a $\\chi^2$ value of 0\n",
    "        * except there's a fraction of 0.71 that might be due to convolution steps/oversampling\n",
    "        * but this is really insignificant, because it's 0.71 of the data out of 1 million\n",
    "* On molecular compounds\n",
    "    * $^{12}\\rm{CO}$ is normal carbon monoxide\n",
    "    * $\\rm{H}^2$ is hydrogen gas (most abundant)\n",
    "    * We don't use $\\rm{H}^2$ for multiple reasons:\n",
    "        * it is pure, does not have rotational transition states due to how symmetric of a molecule it is\n",
    "        * no magnetic moment, has quadropole terms in its moments that are far weaker than monopole/dipole\n",
    "    * Use $^{12}\\rm{CO}$ for multiple reasons:\n",
    "        * Second most abundant element in disks\n",
    "        * Is optically thick\n",
    "            * Has lots of emission from only a little bit of gas\n",
    "        * Bright even at small radii\n",
    "            * easier to reconstruct for velocity measurements/dynamical masses\n",
    "        * Specifically J=2-1 is useful because Earth's atmosphere is most transparent with 2-1\n",
    "            * 1-0 is too hard to observe, is too close to oxygen spectar that is present in Earth's atmosphere\n",
    "            * 3-2 is also used, but (i forget why not)\n",
    "    * We say $^{12}\\rm{CO}$ becuase it is an isotopologue of carbon\n",
    "* On larger scale project:\n",
    "    * We're hoping that post-processing of log-likelihood has a better result than typical practices that don't do this\n",
    "    * Need to stress test model for different scenarios in which to use the model/\n",
    "    * Reduction of data by time-averaging\n",
    "        * help advocate for this way of thinking (exploration), and also helps people make better inferences (inferencing)\n",
    "    * `dynesty` might be better than `emcee` for calculating posteriors (in a more mathematical way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps you took to build an interpolator\n",
    "1. Figured out mechanism behind 1D linear interpolation (literally just $y = mx + b$)\n",
    "2. Built basic version of it\n",
    "3. Tested a dataset that you can control/easily replicate by hand and compared it to other interpolator (`scipy.interpolate.interp1d`)\n",
    "4. Work out any kinks, because your code will not be perfect\n",
    "5. Made functions Jit-able\n",
    "\n",
    "**To do**\n",
    "\n",
    "6. Scale up to real dataset, and make sure output is correct\n",
    "7. Error handling, and making sure conditions are met before interpolation starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba Notes\n",
    "\n",
    "* For a function that takes scalars as input: use decorator `@numba.vectorize` [[source]](https://numba.pydata.org/numba-doc/dev/user/vectorize.html?highlight=vectorize)\n",
    "    * For a function that takes arrays as input (ufunc?): use decorator `@numba.guvectorize`\n",
    "        * Syntax for writing signatures/data types:\n",
    "            * ```@guvectorize([\"void(f8[:], f8[:], c16[:,:], c16[:,:])\"], \"(DVIS_freq), (LSRK_freq), (b, q), (r, s)\", nopython=True)```\n",
    "            * arrays are denoted as \\<datatype>\\[\\<dimensions>\\]\n",
    "                * ex. `c16[:,:]` = 2D array with complex 64-bit digits\n",
    "            * the layout (symbols) for scalars are just `()`\n",
    "            * `void()` means that the function doesn't return anything, otherwise it would be whatever datatype\n",
    "                * useful for `guvectorize`, where you can't make a function return something\n",
    "                    * on this note, can only write data to output array by indexing for `guvectorize`\n",
    "* `@overload` is useful if you have a function that you want to replicate as an unsupported function, and you want Numba to screen the data types of the unsupported function and optimize your jit function [[source]](https://numba.pydata.org/numba-doc/dev/extending/overloading-guide.html?highlight=overload)\n",
    "* `nopython=True` is what does all the JIT compilation for Numba, otherwise it will run in Python compiler (instead of machine-level compiler)\n",
    "* `@njit` makes a function jit-able, with the **n** as shorthand for the `nopython=True` argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10/8 Notes\n",
    "\n",
    "* Next steps: run many different inferences on model, see how outside inferences impact the fit of the model\n",
    "    * But we also want to know what the intrinsic biases of the model are, so that people can make good inferences\n",
    "    * By running no noise into the model, we should be getting back the input parameters that we put in\n",
    "    * By running noise into the model, we find out what the biases of the model are\n",
    "        * Noise is likely to bias the widths (errors) of the parameters, but can also bias the peaks (mean value for distribution)\n",
    "        * We are unsure what the noise should look like (noise is approximated to be gaussian, a la central limit theorem)\n",
    "            * Just a gaussian dist. applied to the real and imaginary visibilities\n",
    "            * In theory, we could run tests on the noise to see how the noise is shaped, but takes too long?\n",
    "\n",
    "\n",
    "* My next steps:\n",
    "    * Modeling corrupted data (via a sampling method, either `emcee` or `dynesty`)\n",
    "        * need to see which one to use first before actually modeling\n",
    "    * Seeing how `dynesty` works for our data\n",
    "        * MCMC (`emcee`) has trouble working with many dimensions (parameters)\n",
    "            * In our case, we have 13-14 mostly independent parametesr\n",
    "            * Can sometimes get stuck in parameter space/all parameters are largely independent from one another, and `emcee` usually works better with more correlated parameters?\n",
    "                * need a way to separate posterior sampling\n",
    "            * `dynesty` uses __nested sampling__ to calculate the convergence of posterior probabilities in parameter space\n",
    "                * does not use random walks, it randomly chooses parameters\n",
    "                    * creates a nest around probable regions, and rejects values outside that 'nest'\n",
    "                * also can calculate priors (evidence) unlike `emcee`\n",
    "            * coding-wise, the documentation/programming is more verbose than `emcee`\n",
    "        * eventually will run this with fake noiseless visibilities to see how it compares to `emcee`\n",
    "            * need to figure out how many points to sample from, if burn-in is required, etc.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on `dynesty`\n",
    "\n",
    "* Dynesty uses **nested sampling** to calculate posterior probability distributions according to Bayesian statistics\n",
    "    * More on __nested sampling__:\n",
    "        * Randomly samples parameter space according to Bayesian __prior__ (defined in a prior function)\n",
    "            * __Prior__ tells us how the parameters are distributed (i.e Gaussian, uniform, etc.)?\n",
    "            * creates bounds based on particular shape (multiple ellipsoids, cubes, spheres, etc.) and draws samples 'live points' (coordinates within prior parameter space)\n",
    "            * Figures out which point has the least likelihood, discards it, and samples from prior again\n",
    "            * By doing this, the algorithm creates a nested shell within prior parameter space where the parameters most likely are\n",
    "            * stops running based on # of points, or other bounding conditions\n",
    "        * Priors must integrate to 1 to be considered 'proper'\n",
    "    * Also provides way to estimate the __Bayesian evidence__ of a posterior\n",
    "        * > The evidence is entirely dependent on the “size” of the prior... using less “informative” priors will increase the expected number of nested sampling iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline\n",
    "---\n",
    "* __Week 11:__ Fundamentals of Statistical Inferencing\n",
    "    * Goal: learn how to use `emcee` and more about MCMC\n",
    "    * _Frustrated about how MCMC works/implementing Metropolis-Hastings_\n",
    "* __Week 12:__ Fake data run through `emcee` and interferometry primer\n",
    "    * Goal: Learn more about interferometry\n",
    "    * Clean up analysis script/work on visualizations\n",
    "    * _Frustrated on how difficult interferometry is_\n",
    "* __Week 13:__ Understand `logL_test.py` and its workings\n",
    "    * Goal: Also learn how to use Numba\n",
    "    * _Frustrated on not knowing what your role in the project is_\n",
    "* __Week 14:__ Numba speed-up\n",
    "    * Goal: Learn how to use Numba\n",
    "    * Goal: Experiment with how we can speed up interpolation\n",
    "    * Completed (10/8): Basic 1D linear interpolator (with simple data set)\n",
    "    * Completed (10/9): Got Numba to run 4x faster than SciPy interpolator (but not too accurate yet)\n",
    "    Feel like I know how to use Numba now!\n",
    "* __Week 15:__ `dynesty` learning\n",
    "    * Goal: Learn how to use `dynesty`\n",
    "    * Goal: Finish interpolator and make it accurate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
