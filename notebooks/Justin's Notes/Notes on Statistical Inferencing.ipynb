{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Statistical Inferencing\n",
    "\n",
    "### Probability\n",
    "* __Multivariate Gaussian density__ (this is similar to the exponential of a single variable gaussian PDF)\n",
    "    <blockquote>$$\n",
    "    p(\\vec{x}) \\propto \\exp \\left [ - \\frac{1}{2} (\\vec{x} -\n",
    "        \\vec{\\mu})^\\mathrm{T} \\, \\Sigma ^{-1} \\, (\\vec{x} - \\vec{\\mu})\n",
    "        \\right ]$$\n",
    "\n",
    "    where $\\vec{\\mu}$ is an $N$-dimensional vector position of the mean of the density and $\\Sigma$ is the square N-by-N covariance matrix.\n",
    "    </blockquote>\n",
    "* __covariance:__ measure of the relationship between two random variables\n",
    "    * $\\sigma(x,y)= \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})$\n",
    "* __variance:__ measure of the spread of a sample/population\n",
    "    * $S^2 = \\sum(x_i - \\bar{x}) \\ / \\ n-1$ (for sample)\n",
    "    * Also, the variance can be described as the covariance of a single variable $\\sigma(x, x)$\n",
    "\n",
    "\n",
    "### Ways to Interpolate data\n",
    "* Most basic MCMC: the __Metropolis-Hastings__ algorithm\n",
    "    * One chain, one walker\n",
    "    * Likelihood function, priors, and proposal distribution (walker)\n",
    "        * Data set has certain mean, variance, standard deviation\n",
    "        * The walker finds the values in parameter space to probe (randomly)\n",
    "        * Feeds the points to likelihood function (usually gaussian) and priors\n",
    "            * Distribution of a likelihood function and priors\n",
    "            * These are multiplied together\n",
    "    * [More on Metropolis-Hastings](https://github.com/Joseph94m/MCMC/blob/master/MCMC.ipynb)\n",
    "* __Linear Least squares:__ finding a line that best fits several linear equations\n",
    "    * $A\\vec{x}=b$ has least squares solutions $A^{T}A\\vec{x}=A^Tb$ (normal equation)\n",
    "    * The least squares solution to $A\\vec{x}=b$ is $x^{*}=(A^TA)^{-1}A^Tb$\n",
    "    * The function takes the form: $f_{\\rm{lsfit}}(s) = x_1f_1(s) + ... + x_nf_n(s)$\n",
    "    * __Linear Least Squares Polynomial Fitting:__ a way of finding a polynomial that best fits several coordinates\n",
    "        * Requires __vandermonde matrix__ \n",
    "            * Matrix takes the form: $$\\begin{equation*}\n",
    "    A = \\begin{pmatrix}\n",
    "      1 & x_0 & x_0^2 & \\cdots & x_0^{n-1} &\n",
    "      x_0^{n}\\\\\n",
    "      1 & x_1 & x_1^2 & \\cdots & x_1^{n-1} &\n",
    "      x_1^{n}\\\\\n",
    "      \\vdots & \\vdots  & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
    "      1 & x_n & x_n^2 & \\cdots & x_n^{n-1} &\n",
    "      x_n^{n}\n",
    "    \\end{pmatrix},\n",
    "    \\end{equation*}$$\n",
    "            * Function takes the form:\n",
    "            $$(a_{1}x^{n-1}+a_{2}x^{n-2}... + a_{n-1}x+a_{n}) = y$$\n",
    "            * Input: x, y points\n",
    "            * Output: fitted polynomial\n",
    "            * __How to (basic)__:\n",
    "                * Points: (0, 1), (1, 0), (2/3, 1/2) ($n$ pairs of coordinates)\n",
    "                * Each row of the matrix takes the form: $(a_{1}x^{n-1}+a_{2}x^{n-2}... + a_{n-1}x+a_{n}) = y$\n",
    "                    * EX: First pair of coordinates would be: $(a_{1}0^{2}+a_{2}0^{1}+a_{3}0^0) =1$\n",
    "                    * Forms augmented matrix:\n",
    "                        * $\\begin{pmatrix} 1&0&0&1\\\\ 1&1&1&0\\\\ 1&\\frac{2}{3}&\\frac{4}{9}&\\frac{1}{2}\\end{pmatrix} \\ $ or, more familiarly,  $\\ \\begin{pmatrix}1&0&0\\\\ 1&1&1\\\\ 1&\\frac{2}{3}&\\frac{4}{9}\\end{pmatrix}\\begin{pmatrix}a_0\\\\ a_1\\\\ a_2\\end{pmatrix}=\\begin{pmatrix}1\\\\ 0\\\\ \\frac{1}{2}\\end{pmatrix}$\n",
    "                    * Solution: $= -\\frac{3}{4}x^2-\\frac{1}{4}x+1$\n",
    "    * Code (from [`emcee` notebook](https://emcee.readthedocs.io/en/stable/tutorials/line/)):\n",
    "```python\n",
    "A = np.vander(x, 2) # make vandermonde matrices (shape 50, 2) out of x values\n",
    "C = np.diag(yerr * yerr) # make diagonal matrix out of square of y-errors\n",
    "ATA = np.dot(A.T, A / (yerr ** 2)[:, None]) #finds A^T•A and divides by error squared (C) shape(2,2)\n",
    "cov = np.linalg.inv(ATA) #inverse\n",
    "w = np.linalg.solve(ATA, np.dot(A.T, y / yerr ** 2)) #this is least-squares solution\n",
    "print(\"Least-squares estimates:\")\n",
    "print(\"m = {0:.3f} ± {1:.3f}\".format(w[0], np.sqrt(cov[0, 0]))) #error is sqrt of diagonal of covariance matrix\n",
    "print(\"b = {0:.3f} ± {1:.3f}\".format(w[1], np.sqrt(cov[1, 1])))\n",
    "```\n",
    "    * In math notation:\n",
    "        * $AX=Y$ with solution $w=x^*=[A^T C^{-1} A]^{-1}[A^TC^{-1}Y]$\n",
    "            * $A$ = vandermonde matrix\n",
    "            * $C^{-1}$ = divided by __covariance matrix__: $C(m, b) = \\begin{pmatrix}\\sigma_{b}^2&\\sigma_{mb}^2\\\\ \\sigma_{mb}^2&\\sigma_{m}^2\\end{pmatrix}$\n",
    "                * in the code, $C$ is determined by the diagonal of the error in y\n",
    "            * $Y$ = distribution of y values (with error and added fractional uncertainty)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/14 Meeting Notes\n",
    "\n",
    "* Dr. Andrews/Teague have been working on statistical inferencing of the spectral line model\n",
    "    * __Statistical inferencing:__ statistical techniques (in this case, Bayesian analysis) that tell us how well the model measures individual parameters ($m_{*}$, $\\rm{T}_b$, $\\Delta v$, etc.)\n",
    "        * Posterior Probability- Bayesian probability of the accuracy of the models given data/outside information\n",
    "            * __Bayes Theorem:__ $$\\Pr ( M \\mid D ) = \\Pr(D \\mid M) * \\Pr(M) \\ / \\Pr(D)$$\n",
    "            * _In words: The probability of the model given the data is equal to the probability of the data given the model (__likelihood__) multiplied by the probability of the data (__prior__) divided by the probability of the data (__Bayesian evidence__)_\n",
    "                * __likelihood:__ ($\\rm{L}$) The probability of the model given the data\n",
    "                    * statistical inferencing is done in natural log because it is assumed that probabilities are gaussian, and the natural log of a gaussian is ~ roughly ~ the area under the curve\n",
    "                    * Found through __chi-squared analysis__\n",
    "                        * $\\chi^2=(D-M)^2 \\ / \\ (\\rm{noise})$\n",
    "                            * roughly, subtract model channel maps from data channel maps\n",
    "                        * $\\ln(\\rm{L}) = \\Sigma_{i=0} = -\\chi^2 \\ / \\ 2$ (__double check__)\n",
    "                            * sum over visibilities (for now, all channel maps for data and model)\n",
    "                * __prior:__ $(\\Pr(M))$ the assumptions we make about the data\n",
    "                    * assumptions like the range of the data (e.g. $0-1000 \\rm{K}$), the gaussian distribution of the data (e.g. $370 \\pm 10 \\ \\rm{K}$)\n",
    "                * __Bayesian evidence:__ $(\\Pr(D))$ hard to measure, but not necessary\n",
    "                    * __MCMC algorithms__ do not need this quantity to work\n",
    "        * In reality, $\\Pr ( M \\mid D ) \\sim \\Pr(D \\mid M) * \\Pr(M) \\ / \\Pr(D)$\n",
    "            * Because $\\Pr(D)$ is hard to measure, we use __Markov Chain Monte Carlo (MCMC)__ to weigh this bayesian probability equation in relation to proportions of the numerator, not exact values\n",
    "            * __Markov Chain Monte Carlo__  uses a __Markov chain__ to sample probability at random points (random walks)\n",
    "                * A __Monte Carlo__ algorithm is a way of randomly sampling a distribution to estimate the distributions of parameters given a set of observations\n",
    "                * A __Markov chain__ looks at the ratio of probabilities between current chain and last chain to determine where to sample probability next\n",
    "                    * More specifically, the next sample is determined by the correlation between the area of the PDF and the ratio\n",
    "                        * Algorithm finds ratios that contribute significantly to the area of the PDF\n",
    "                * A variant of MCMC (that will most likely be the algorithm used on models) is the __affine invariant ensemble sampler__ (`emcee` package)\n",
    "                    * Essentially, multiple Markov chains occuring at once for faster processing\n",
    "                        * Can specify number of chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/21 Notes\n",
    "* Code that provides an output for 'fake data' `analyze_fit_emcee.py`\n",
    "    * Goal: do we recover what we put in? We know the truth for the parameters, but does emcee handle everything correctly?\n",
    "    * Fake data is generated with _no noise_ (no noise meaning that the typical noise from an interferometer is not accounted for (in the form of gaussians), only vague uncertainties\n",
    "    * Code is slow on two accounts:\n",
    "        * Making a cube to store data\n",
    "            * __broadcasting__ arrays: handling arrays of different sizes for computation (NumPy)\n",
    "                * i.e. matrix * scalar number (larger than 1)\n",
    "            * computationally taxing\n",
    "        * Post processing/interpolation for 1e6 different arrays for the likelihood calculation\n",
    "        * Is possible to make it faster using Numba\n",
    "            * Basically takes python code and makes it faster—cluster/machine optimized\n",
    "    \n",
    "* __Next steps:__\n",
    "    * Testing a real dataset\n",
    "        * Advantage: figure out what the errors are, sooner\n",
    "        * Disadvantage: we don't know 'the truth'/what we're looking for, so we are subject to lots of biases\n",
    "    * Technical biases\n",
    "        * Training data to conform to the truth\n",
    "        * make certain calibrations to improve computational speed\n",
    "            * time-averaging: taking larger samples in time (i.e not ever 6 seconds, but every 30 seconds)\n",
    "                * Question: Does it make sense to time-average? Do we still retain the same amount of information? __how to optimize__\n",
    "\n",
    "* __Long term:__\n",
    "    * Is it best to find out ideal conditions to collect interferometer data to probe for disk masses?\n",
    "    * Or, is it best to figure out what a model of dynamical masses actually looks like and the errors associated with it?\n",
    "    * Exploring both simultaneously (Teague, Andrews, and myself)\n",
    "\n",
    "* __Miscellaneous:__\n",
    "    * Spectral resolution: frequency/velocity at which you take measurements of the sky with interferometry\n",
    "        * One technique to increase resolution: VLBI (Very-Long Baseline Interferometry)\n",
    "    * Optimum data collection: higher spectral resolution\n",
    "        * Caveats:\n",
    "            * Telescope time is competitive\n",
    "                * Observations fall into two camps:\n",
    "                    * Snapshot (1 sec-1 min of sky)\n",
    "                    * Medium length (20-30 minutes)\n",
    "                    * Long (1-5 hrs)\n",
    "            * Higher spectral resolution = higher noise (similar to fourier transform analysis)\n",
    "    * Spectral continuum = 1 channel map\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline\n",
    "---\n",
    "* __Week 11:__ Fundamentals of Statistical Inferencing\n",
    "    * Goal: learn how to use `emcee` and more about MCMC\n",
    "* __Week 12:__ Fake data run through `emcee` and interferometry primer\n",
    "    * Goal: Learn more about interferometry\n",
    "    * Clean up analysis script/work on visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
